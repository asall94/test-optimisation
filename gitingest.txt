================================================
FILE: README.md
================================================
# Infrastructure Monitoring & Optimization Agent

## Agent Architecture

Autonomous multi-node LangGraph agent implementing intelligent infrastructure analysis through sequential reasoning:

```mermaid
graph LR
    A[rapport.json<br/>10k+ snapshots] --> B[Agent Node 1: Ingestion<br/>Parse & Validate]
    B --> C[Agent Node 2: Analysis<br/>Compute Insights<br/>Detect Anomalies]
    C --> D[Agent Node 3: LLM Enrichment<br/>Generate Recommendations]
    D --> E[Agent Node 4: Output<br/>Format JSON Report]
    E --> F[output.json]
    
    style A fill:#e1f5ff
    style F fill:#d4edda
    style B fill:#fff3cd
    style C fill:#fff3cd
    style D fill:#f8d7da
    style E fill:#fff3cd
```

**Agent Flow:**
```
Ingestion → Analysis → LLM Enrichment → Output
```

### Agent Node Functions

1. **Ingestion Node**: Loads and validates 10k+ monitoring snapshots from JSON
2. **Analysis Node**: Computes aggregated insights and detects SLA violations via threshold-based detection
3. **LLM Enrichment Node**: Generates contextual recommendations using OpenAI GPT-4
4. **Output Node**: Exports structured JSON report conforming to specification schema

## Agentic Design

This solution implements an **autonomous agent** using LangGraph's state machine architecture:

- **State Management**: Shared GraphState passed between nodes enables context retention
- **Sequential Reasoning**: Each node performs specialized analysis before passing to next stage
- **LLM Integration**: Agent leverages GPT-4 for intelligent recommendation generation
- **Deterministic + Generative**: Combines rule-based anomaly detection with AI-powered insights

## Technical Stack

- **LangGraph**: Multi-node agent orchestration and state management
- **Pydantic**: Schema validation and type safety
- **OpenAI GPT-4**: LLM-powered contextual reasoning
- **Python 3.9+**: Core implementation language

## Installation

```powershell
pip install -r requirements.txt
```

Create `.env` file:
```
OPENAI_API_KEY=your_key_here
```

## Execution

```powershell
python main.py
```

Output: `output.json`

## Design Decisions

### Anomaly Detection
- **Current**: Statistical thresholds based on business SLAs (CPU > 85%, latency > 250ms, etc.)
- **Production**: ML-based models (Isolation Forest, ARIMA for time-series)

### Recommendations
- **Current**: LLM-generated for contextual, business-oriented actions
- **Production**: Hybrid rule engine + LLM fallback for cost optimization

### Data Processing
- **Current**: In-memory batch processing
- **Production**: Streaming architecture (Kafka/Kinesis) with incremental aggregation

### Scalability Considerations
- Deduplication logic prevents anomaly explosion
- Severity scoring prioritizes critical issues
- JSON schema validation ensures output contract compliance

## Code Structure

```
main.py           - Agent orchestration and entry point
nodes.py          - Agent node implementations
models.py         - Pydantic data schemas
rapport.json      - Input monitoring data (10k+ snapshots)
output.json       - Generated analysis report
test_pipeline.py  - Unit tests for agent nodes
validate.py       - Output schema validation
```

## Business Value

- **Autonomous Analysis**: Agent processes infrastructure data without human intervention
- **Intelligent Recommendations**: LLM-generated actions provide contextual optimization paths
- **MTTR Reduction**: Automated anomaly detection accelerates incident response
- **System Integration**: Structured output enables alerting/ticketing system integration



================================================
FILE: main.py
================================================
"""
Infrastructure Monitoring & Optimization Agent

Multi-node LangGraph agent for autonomous infrastructure analysis:
1. Data ingestion and validation
2. Statistical analysis and anomaly detection  
3. LLM-powered recommendation generation
4. Structured JSON report output

Technical choices:
- LangGraph: Orchestrates agent nodes with state management
- Pydantic: Enforces schema validation (input/output integrity)
- OpenAI GPT-4: Generates contextual, business-oriented recommendations
- Python: Rapid development with strong data processing ecosystem

Usage:
1. Set OPENAI_API_KEY in .env file
2. Run: python main.py
3. Output: output.json
"""

import os
from dotenv import load_dotenv
from langgraph.graph import StateGraph, END
from nodes import (
    GraphState,
    ingestion_node,
    analysis_node,
    llm_enrichment_node,
    output_node
)


def create_workflow() -> StateGraph:
    """
    Builds the LangGraph agent with 4 sequential nodes.
    
    Agent Flow: Ingestion -> Analysis -> LLM Enrichment -> Output
    """
    workflow = StateGraph(GraphState)
    
    # Define nodes
    workflow.add_node("ingestion", ingestion_node)
    workflow.add_node("analysis", analysis_node)
    workflow.add_node("llm_enrichment", llm_enrichment_node)
    workflow.add_node("generate_output", output_node)
    
    # Define edges (sequential processing)
    workflow.set_entry_point("ingestion")
    workflow.add_edge("ingestion", "analysis")
    workflow.add_edge("analysis", "llm_enrichment")
    workflow.add_edge("llm_enrichment", "generate_output")
    workflow.add_edge("generate_output", END)
    
    return workflow.compile()


def main():
    """Execute the monitoring agent."""
    # Load environment variables
    load_dotenv()
    
    if not os.getenv("OPENAI_API_KEY"):
        raise ValueError("OPENAI_API_KEY not found. Create .env file with API key.")
    
    print("Starting infrastructure monitoring agent...")
    
    # Initialize and run agent
    app = create_workflow()
    
    # Execute with empty initial state
    result = app.invoke({
        "raw_data": [],
        "parsed_data": [],
        "insights": None,
        "anomalies": [],
        "recommendations": [],
        "service_status_summary": None,
        "output": None
    })
    
    print(f"\nAgent execution completed successfully.")
    print(f"Processed {len(result['parsed_data'])} data points")
    print(f"Detected {len(result['anomalies'])} anomalies")
    print(f"Generated {len(result['recommendations'])} recommendations")
    print(f"\nOutput saved to: output.json")


if __name__ == "__main__":
    main()



================================================
FILE: models.py
================================================
"""
Data models for infrastructure monitoring agent.
Defines input/output schemas with strict validation.
"""

from pydantic import BaseModel, Field
from typing import List, Dict, Literal
from datetime import datetime


class ServiceStatus(BaseModel):
    """Service health status."""
    database: str
    api_gateway: str
    cache: str


class InputData(BaseModel):
    """Single monitoring snapshot from infrastructure."""
    timestamp: str
    cpu_usage: float
    memory_usage: float
    latency_ms: float
    disk_usage: float
    network_in_kbps: float
    network_out_kbps: float
    io_wait: float
    thread_count: int
    active_connections: int
    error_rate: float
    uptime_seconds: int
    temperature_celsius: float
    power_consumption_watts: float
    service_status: ServiceStatus


class Insights(BaseModel):
    """Aggregated metrics insights."""
    average_latency_ms: float
    max_cpu_usage: float
    max_memory_usage: float
    error_rate: float
    uptime_seconds: int


class Anomaly(BaseModel):
    """Detected infrastructure anomaly."""
    metric: str
    value: float
    threshold: float
    severity: Literal["low", "medium", "high"]
    description: str


class Recommendation(BaseModel):
    """Action recommendation for optimization."""
    id: str
    action: str
    target: str
    parameters: Dict
    benefit_estimate: str


class ServiceStatusSummary(BaseModel):
    """Summary of service health across all snapshots."""
    online: List[str]
    degraded: List[str]
    offline: List[str]


class OutputData(BaseModel):
    """Final analysis output schema."""
    timestamp: str
    insights: Insights
    anomalies: List[Anomaly]
    recommendations: List[Recommendation]
    service_status_summary: ServiceStatusSummary



================================================
FILE: nodes.py
================================================
"""
LangGraph agent nodes for infrastructure monitoring analysis.

Agent Architecture:
1. Ingestion Node: Load and validate input data
2. Analysis Node: Compute insights and detect anomalies
3. LLM Enrichment Node: Generate contextual recommendations
4. Output Node: Format final JSON report

Production considerations:
- Anomaly detection uses statistical thresholds (prod: ML-based models like Isolation Forest)
- LLM for recommendations (prod: hybrid approach with rule engine + LLM fallback)
- In-memory processing (prod: streaming architecture with Kafka/Kinesis)
"""

import json
import logging
import statistics
from typing import List, Dict, Any, TypedDict
from datetime import datetime
from models import InputData, Insights, Anomaly, Recommendation, ServiceStatusSummary, OutputData
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage


# Configure structured JSON logging
logging.basicConfig(
    level=logging.INFO,
    format='{"timestamp": "%(asctime)s", "level": "%(levelname)s", "module": "%(module)s", "message": "%(message)s"}',
    datefmt='%Y-%m-%dT%H:%M:%SZ'
)
logger = logging.getLogger(__name__)


class GraphState(TypedDict):
    """State passed between workflow nodes."""
    raw_data: List[Dict[str, Any]]
    parsed_data: List[InputData]
    insights: Insights
    anomalies: List[Anomaly]
    recommendations: List[Recommendation]
    service_status_summary: ServiceStatusSummary
    output: OutputData


# Anomaly detection thresholds (business-defined SLAs)
THRESHOLDS = {
    "cpu_usage": {"high": 85, "medium": 75},
    "memory_usage": {"high": 80, "medium": 70},
    "latency_ms": {"high": 250, "medium": 180},
    "error_rate": {"high": 0.05, "medium": 0.02},
    "temperature_celsius": {"high": 75, "medium": 65},
    "io_wait": {"high": 10, "medium": 7},
}


def ingestion_node(state: GraphState) -> GraphState:
    """
    Node 1: Load and validate input data.
    Ensures data integrity before processing.
    """
    logger.info("Starting data ingestion")
    
    with open("rapport.json", "r") as f:
        raw_data = json.load(f)
    
    parsed_data = [InputData(**entry) for entry in raw_data]
    
    logger.info(f"Ingestion complete: {len(parsed_data)} snapshots loaded")
    
    state["raw_data"] = raw_data
    state["parsed_data"] = parsed_data
    return state


def analysis_node(state: GraphState) -> GraphState:
    """
    Node 2: Compute insights and detect anomalies.
    
    Insights: Aggregated metrics across all snapshots
    Anomalies: Threshold-based detection with severity scoring
    """
    logger.info("Starting analysis phase")
    data = state["parsed_data"]
    
    # Compute insights
    latencies = [d.latency_ms for d in data]
    cpu_usages = [d.cpu_usage for d in data]
    memory_usages = [d.memory_usage for d in data]
    error_rates = [d.error_rate for d in data]
    
    insights = Insights(
        average_latency_ms=round(statistics.mean(latencies), 2),
        max_cpu_usage=round(max(cpu_usages), 2),
        max_memory_usage=round(max(memory_usages), 2),
        error_rate=round(statistics.mean(error_rates), 4),
        uptime_seconds=data[-1].uptime_seconds
    )
    
    # Detect anomalies (deduplication: keep worst case per metric)
    anomaly_map: Dict[str, Anomaly] = {}
    
    for entry in data:
        # CPU anomaly
        if entry.cpu_usage > THRESHOLDS["cpu_usage"]["high"]:
            severity = "high"
            if "cpu_usage" not in anomaly_map or entry.cpu_usage > anomaly_map["cpu_usage"].value:
                anomaly_map["cpu_usage"] = Anomaly(
                    metric="cpu_usage",
                    value=entry.cpu_usage,
                    threshold=THRESHOLDS["cpu_usage"]["high"],
                    severity=severity,
                    description=f"CPU usage reached {entry.cpu_usage}% at {entry.timestamp}"
                )
        elif entry.cpu_usage > THRESHOLDS["cpu_usage"]["medium"]:
            if "cpu_usage" not in anomaly_map:
                anomaly_map["cpu_usage"] = Anomaly(
                    metric="cpu_usage",
                    value=entry.cpu_usage,
                    threshold=THRESHOLDS["cpu_usage"]["medium"],
                    severity="medium",
                    description=f"CPU usage elevated at {entry.cpu_usage}%"
                )
        
        # Memory anomaly
        if entry.memory_usage > THRESHOLDS["memory_usage"]["high"]:
            if "memory_usage" not in anomaly_map or entry.memory_usage > anomaly_map["memory_usage"].value:
                anomaly_map["memory_usage"] = Anomaly(
                    metric="memory_usage",
                    value=entry.memory_usage,
                    threshold=THRESHOLDS["memory_usage"]["high"],
                    severity="high",
                    description=f"Memory usage critical at {entry.memory_usage}%"
                )
        
        # Latency anomaly
        if entry.latency_ms > THRESHOLDS["latency_ms"]["high"]:
            if "latency_ms" not in anomaly_map or entry.latency_ms > anomaly_map["latency_ms"].value:
                anomaly_map["latency_ms"] = Anomaly(
                    metric="latency_ms",
                    value=entry.latency_ms,
                    threshold=THRESHOLDS["latency_ms"]["high"],
                    severity="high",
                    description=f"Latency spike to {entry.latency_ms}ms"
                )
        
        # Error rate anomaly
        if entry.error_rate > THRESHOLDS["error_rate"]["high"]:
            if "error_rate" not in anomaly_map or entry.error_rate > anomaly_map["error_rate"].value:
                anomaly_map["error_rate"] = Anomaly(
                    metric="error_rate",
                    value=entry.error_rate,
                    threshold=THRESHOLDS["error_rate"]["high"],
                    severity="high",
                    description=f"Error rate critical at {entry.error_rate*100}%"
                )
        
        # Temperature anomaly
        if entry.temperature_celsius > THRESHOLDS["temperature_celsius"]["high"]:
            if "temperature_celsius" not in anomaly_map or entry.temperature_celsius > anomaly_map["temperature_celsius"].value:
                anomaly_map["temperature_celsius"] = Anomaly(
                    metric="temperature_celsius",
                    value=entry.temperature_celsius,
                    threshold=THRESHOLDS["temperature_celsius"]["high"],
                    severity="high",
                    description=f"Server temperature at {entry.temperature_celsius}C"
                )
        
        # IO wait anomaly
        if entry.io_wait > THRESHOLDS["io_wait"]["high"]:
            if "io_wait" not in anomaly_map or entry.io_wait > anomaly_map["io_wait"].value:
                anomaly_map["io_wait"] = Anomaly(
                    metric="io_wait",
                    value=entry.io_wait,
                    threshold=THRESHOLDS["io_wait"]["high"],
                    severity="medium",
                    description=f"IO wait time elevated at {entry.io_wait}%"
                )
    
    # Service status aggregation
    status_tracker = {"online": set(), "degraded": set(), "offline": set()}
    for entry in data:
        for service, status in entry.service_status.model_dump().items():
            status_tracker[status].add(service)
    
    # Remove duplicates across statuses (prioritize worst state)
    if status_tracker["offline"]:
        for service in status_tracker["offline"]:
            status_tracker["degraded"].discard(service)
            status_tracker["online"].discard(service)
    if status_tracker["degraded"]:
        for service in status_tracker["degraded"]:
            status_tracker["online"].discard(service)
    
    service_status_summary = ServiceStatusSummary(
        online=sorted(list(status_tracker["online"])),
        degraded=sorted(list(status_tracker["degraded"])),
        offline=sorted(list(status_tracker["offline"]))
    )
    
    logger.info(f"Analysis complete: {len(list(anomaly_map.values()))} anomalies detected")
    
    state["insights"] = insights
    state["anomalies"] = list(anomaly_map.values())
    state["service_status_summary"] = service_status_summary
    return state


def llm_enrichment_node(state: GraphState) -> GraphState:
    """
    Node 3: Generate contextual recommendations using LLM.
    
    Leverages OpenAI to produce business-oriented, actionable recommendations
    based on detected anomalies and infrastructure patterns.
    """
    logger.info("Starting LLM enrichment phase")
    
    anomalies = state["anomalies"]
    insights = state["insights"]
    
    # Build context for LLM
    anomaly_summary = "\n".join([
        f"- {a.metric}: {a.value} (threshold: {a.threshold}, severity: {a.severity})"
        for a in anomalies
    ])
    
    system_prompt = """You are an infrastructure optimization expert for a French SME.
Generate 3-5 precise technical recommendations based on detected anomalies.
Each recommendation must be actionable, specific, and estimate business impact.

Respond ONLY with valid JSON (no markdown, no code blocks):
[
  {
    "id": "rec-001",
    "action": "specific technical action",
    "target": "affected component",
    "parameters": {"key": "value"},
    "benefit_estimate": "quantified business benefit"
  }
]"""
    
    user_prompt = f"""Infrastructure analysis summary:

Anomalies detected:
{anomaly_summary if anomaly_summary else "No critical anomalies"}

Key metrics:
- Average latency: {insights.average_latency_ms}ms
- Max CPU: {insights.max_cpu_usage}%
- Max memory: {insights.max_memory_usage}%
- Error rate: {insights.error_rate*100}%

Generate optimization recommendations in JSON format."""
    
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    response = llm.invoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])
    
    # Parse LLM response
    recommendations_data = json.loads(response.content)
    recommendations = [Recommendation(**rec) for rec in recommendations_data]
    
    logger.info(f"LLM enrichment complete: {len(recommendations)} recommendations generated")
    
    state["recommendations"] = recommendations
    return state


def output_node(state: GraphState) -> GraphState:
    """
    Node 4: Format and export final analysis report.
    """
    logger.info("Generating output report")
    
    output = OutputData(
        timestamp=datetime.utcnow().isoformat() + "Z",
        insights=state["insights"],
        anomalies=state["anomalies"],
        recommendations=state["recommendations"],
        service_status_summary=state["service_status_summary"]
    )
    
    # Write to output.json
    with open("output.json", "w", encoding="utf-8") as f:
        json.dump(output.model_dump(), f, indent=2, ensure_ascii=False)
    
    logger.info("Output report saved to output.json")
    
    state["output"] = output
    return state



================================================
FILE: requirements.txt
================================================
langgraph==0.2.28
langchain==0.3.0
langchain-openai==0.2.0
pydantic==2.9.2
python-dotenv==1.0.1



================================================
FILE: test_pipeline.py
================================================
"""
Unit tests for infrastructure monitoring agent.
Tests core functionality: data ingestion, anomaly detection, and output formatting.
"""

import unittest
import json
from models import InputData, Insights, Anomaly, Recommendation, ServiceStatusSummary, OutputData
from nodes import ingestion_node, analysis_node, GraphState, THRESHOLDS


class TestDataIngestion(unittest.TestCase):
    """Test data loading and validation."""
    
    def test_input_data_parsing(self):
        """Verify InputData model parses valid JSON correctly."""
        sample = {
            "timestamp": "2023-10-01T12:00:00Z",
            "cpu_usage": 85,
            "memory_usage": 70,
            "latency_ms": 250,
            "disk_usage": 65,
            "network_in_kbps": 1200,
            "network_out_kbps": 900,
            "io_wait": 5,
            "thread_count": 150,
            "active_connections": 45,
            "error_rate": 0.02,
            "uptime_seconds": 360000,
            "temperature_celsius": 65,
            "power_consumption_watts": 250,
            "service_status": {
                "database": "online",
                "api_gateway": "degraded",
                "cache": "online"
            }
        }
        
        data = InputData(**sample)
        self.assertEqual(data.cpu_usage, 85)
        self.assertEqual(data.service_status.database, "online")
    
    def test_ingestion_node_loads_data(self):
        """Verify ingestion node loads rapport.json successfully."""
        state = GraphState(
            raw_data=[],
            parsed_data=[],
            insights=None,
            anomalies=[],
            recommendations=[],
            service_status_summary=None,
            output=None
        )
        
        result = ingestion_node(state)
        self.assertGreater(len(result["parsed_data"]), 0)
        self.assertIsInstance(result["parsed_data"][0], InputData)


class TestAnomalyDetection(unittest.TestCase):
    """Test anomaly detection logic."""
    
    def test_cpu_threshold_detection(self):
        """Verify CPU anomalies are detected correctly."""
        test_data = [
            InputData(
                timestamp="2023-10-01T12:00:00Z",
                cpu_usage=95,  # Above high threshold
                memory_usage=60,
                latency_ms=100,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=3,
                thread_count=100,
                active_connections=30,
                error_rate=0.01,
                uptime_seconds=10000,
                temperature_celsius=60,
                power_consumption_watts=200,
                service_status={"database": "online", "api_gateway": "online", "cache": "online"}
            )
        ]
        
        state = GraphState(
            raw_data=[],
            parsed_data=test_data,
            insights=None,
            anomalies=[],
            recommendations=[],
            service_status_summary=None,
            output=None
        )
        
        result = analysis_node(state)
        cpu_anomalies = [a for a in result["anomalies"] if a.metric == "cpu_usage"]
        
        self.assertEqual(len(cpu_anomalies), 1)
        self.assertEqual(cpu_anomalies[0].severity, "high")
        self.assertEqual(cpu_anomalies[0].value, 95)
    
    def test_multiple_anomaly_detection(self):
        """Verify multiple simultaneous anomalies are detected."""
        test_data = [
            InputData(
                timestamp="2023-10-01T12:00:00Z",
                cpu_usage=90,
                memory_usage=85,
                latency_ms=300,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=12,
                thread_count=100,
                active_connections=30,
                error_rate=0.08,
                uptime_seconds=10000,
                temperature_celsius=80,
                power_consumption_watts=200,
                service_status={"database": "online", "api_gateway": "online", "cache": "online"}
            )
        ]
        
        state = GraphState(
            raw_data=[],
            parsed_data=test_data,
            insights=None,
            anomalies=[],
            recommendations=[],
            service_status_summary=None,
            output=None
        )
        
        result = analysis_node(state)
        
        # Should detect: CPU, memory, latency, error_rate, temperature, io_wait
        self.assertGreaterEqual(len(result["anomalies"]), 5)
        
        detected_metrics = {a.metric for a in result["anomalies"]}
        self.assertIn("cpu_usage", detected_metrics)
        self.assertIn("memory_usage", detected_metrics)
        self.assertIn("latency_ms", detected_metrics)
    
    def test_no_anomalies_when_within_thresholds(self):
        """Verify no anomalies detected when all metrics are normal."""
        test_data = [
            InputData(
                timestamp="2023-10-01T12:00:00Z",
                cpu_usage=50,
                memory_usage=60,
                latency_ms=100,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=3,
                thread_count=100,
                active_connections=30,
                error_rate=0.01,
                uptime_seconds=10000,
                temperature_celsius=60,
                power_consumption_watts=200,
                service_status={"database": "online", "api_gateway": "online", "cache": "online"}
            )
        ]
        
        state = GraphState(
            raw_data=[],
            parsed_data=test_data,
            insights=None,
            anomalies=[],
            recommendations=[],
            service_status_summary=None,
            output=None
        )
        
        result = analysis_node(state)
        self.assertEqual(len(result["anomalies"]), 0)


class TestInsightsCalculation(unittest.TestCase):
    """Test metrics aggregation and insights computation."""
    
    def test_insights_aggregation(self):
        """Verify insights are calculated correctly from multiple data points."""
        test_data = [
            InputData(
                timestamp="2023-10-01T12:00:00Z",
                cpu_usage=80,
                memory_usage=70,
                latency_ms=200,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=3,
                thread_count=100,
                active_connections=30,
                error_rate=0.02,
                uptime_seconds=10000,
                temperature_celsius=60,
                power_consumption_watts=200,
                service_status={"database": "online", "api_gateway": "online", "cache": "online"}
            ),
            InputData(
                timestamp="2023-10-01T12:30:00Z",
                cpu_usage=90,
                memory_usage=75,
                latency_ms=150,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=3,
                thread_count=100,
                active_connections=30,
                error_rate=0.03,
                uptime_seconds=11800,
                temperature_celsius=60,
                power_consumption_watts=200,
                service_status={"database": "online", "api_gateway": "online", "cache": "online"}
            )
        ]
        
        state = GraphState(
            raw_data=[],
            parsed_data=test_data,
            insights=None,
            anomalies=[],
            recommendations=[],
            service_status_summary=None,
            output=None
        )
        
        result = analysis_node(state)
        insights = result["insights"]
        
        self.assertEqual(insights.average_latency_ms, 175.0)
        self.assertEqual(insights.max_cpu_usage, 90.0)
        self.assertEqual(insights.max_memory_usage, 75.0)
        self.assertEqual(insights.uptime_seconds, 11800)


class TestServiceStatusAggregation(unittest.TestCase):
    """Test service health status tracking."""
    
    def test_service_status_prioritization(self):
        """Verify worst service status is prioritized correctly."""
        test_data = [
            InputData(
                timestamp="2023-10-01T12:00:00Z",
                cpu_usage=50,
                memory_usage=60,
                latency_ms=100,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=3,
                thread_count=100,
                active_connections=30,
                error_rate=0.01,
                uptime_seconds=10000,
                temperature_celsius=60,
                power_consumption_watts=200,
                service_status={"database": "online", "api_gateway": "degraded", "cache": "online"}
            ),
            InputData(
                timestamp="2023-10-01T12:30:00Z",
                cpu_usage=50,
                memory_usage=60,
                latency_ms=100,
                disk_usage=50,
                network_in_kbps=1000,
                network_out_kbps=800,
                io_wait=3,
                thread_count=100,
                active_connections=30,
                error_rate=0.01,
                uptime_seconds=11800,
                temperature_celsius=60,
                power_consumption_watts=200,
                service_status={"database": "offline", "api_gateway": "online", "cache": "degraded"}
            )
        ]
        
        state = GraphState(
            raw_data=[],
            parsed_data=test_data,
            insights=None,
            anomalies=[],
            recommendations=[],
            service_status_summary=None,
            output=None
        )
        
        result = analysis_node(state)
        status_summary = result["service_status_summary"]
        
        # Database should be offline (worst state)
        self.assertIn("database", status_summary.offline)
        self.assertNotIn("database", status_summary.online)
        self.assertNotIn("database", status_summary.degraded)
        
        # API gateway should be degraded
        self.assertIn("api_gateway", status_summary.degraded)
        
        # Cache should be degraded
        self.assertIn("cache", status_summary.degraded)


class TestOutputSchema(unittest.TestCase):
    """Test output data model validation."""
    
    def test_output_schema_compliance(self):
        """Verify OutputData model matches specification."""
        output_data = {
            "timestamp": "2023-10-01T12:00:00Z",
            "insights": {
                "average_latency_ms": 150.5,
                "max_cpu_usage": 90.0,
                "max_memory_usage": 80.0,
                "error_rate": 0.02,
                "uptime_seconds": 360000
            },
            "anomalies": [
                {
                    "metric": "cpu_usage",
                    "value": 90.0,
                    "threshold": 85.0,
                    "severity": "high",
                    "description": "CPU usage critical"
                }
            ],
            "recommendations": [
                {
                    "id": "rec-001",
                    "action": "scale up resources",
                    "target": "server",
                    "parameters": {"cores": 4},
                    "benefit_estimate": "30% improvement"
                }
            ],
            "service_status_summary": {
                "online": ["cache"],
                "degraded": ["api_gateway"],
                "offline": ["database"]
            }
        }
        
        output = OutputData(**output_data)
        
        self.assertEqual(output.timestamp, "2023-10-01T12:00:00Z")
        self.assertEqual(len(output.anomalies), 1)
        self.assertEqual(len(output.recommendations), 1)
        self.assertEqual(output.anomalies[0].severity, "high")


if __name__ == "__main__":
    unittest.main(verbosity=2)



================================================
FILE: validate.py
================================================
"""
Validation script for output.json schema compliance.
Ensures generated report matches specification requirements.
"""

import json
from models import OutputData


def validate_output():
    """Validate output.json against specification schema."""
    
    print("Validating output.json...")
    
    try:
        # Load generated output
        with open("output.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # Validate using Pydantic model
        output = OutputData(**data)
        
        # Schema validation checks
        checks = {
            "timestamp_format": output.timestamp.endswith("Z") and "T" in output.timestamp,
            "insights_complete": all([
                output.insights.average_latency_ms >= 0,
                output.insights.max_cpu_usage >= 0,
                output.insights.max_memory_usage >= 0,
                output.insights.error_rate >= 0,
                output.insights.uptime_seconds >= 0
            ]),
            "anomalies_valid": all([
                a.severity in ["low", "medium", "high"] for a in output.anomalies
            ]),
            "recommendations_valid": all([
                len(r.id) > 0 and len(r.action) > 0 for r in output.recommendations
            ]),
            "service_status_valid": (
                isinstance(output.service_status_summary.online, list) and
                isinstance(output.service_status_summary.degraded, list) and
                isinstance(output.service_status_summary.offline, list)
            )
        }
        
        # Report results
        print("\n--- Validation Results ---")
        for check_name, passed in checks.items():
            status = "✓ PASS" if passed else "✗ FAIL"
            print(f"{status}: {check_name}")
        
        if all(checks.values()):
            print("\n✓ output.json is fully compliant with specification")
            print(f"\nMetrics:")
            print(f"  - Anomalies detected: {len(output.anomalies)}")
            print(f"  - Recommendations generated: {len(output.recommendations)}")
            print(f"  - Services online: {len(output.service_status_summary.online)}")
            print(f"  - Services degraded: {len(output.service_status_summary.degraded)}")
            print(f"  - Services offline: {len(output.service_status_summary.offline)}")
            return True
        else:
            print("\n✗ Validation failed - see errors above")
            return False
            
    except Exception as e:
        print(f"\n✗ Validation error: {str(e)}")
        return False


if __name__ == "__main__":
    validate_output()



================================================
FILE: .env.example
================================================
OPENAI_API_KEY=your_openai_api_key_here


